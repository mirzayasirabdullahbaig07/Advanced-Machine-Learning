"""
# Gradient Descent (Scratch + Animation + Variants)
# Author: Yasir Abdullah

üìò Concept Overview:
-------------------
Gradient Descent (GD) is an optimization algorithm used to minimize a **cost function** 
by iteratively moving in the direction of the **negative gradient** of that function.

It‚Äôs widely used in:
- Linear Regression
- Logistic Regression
- Deep Learning (to train neural networks)
- Optimization tasks across AI/ML

---

## Why Gradient Descent is Used:
Because many ML models involve parameters (like weights and biases) that 
must be optimized to minimize error (loss). Exact analytical solutions 
are not always possible ‚Äî GD finds the optimal parameters iteratively.

---

## Formula:
For any parameter Œ∏ (could be slope `m` or intercept `b`):
Œ∏_new = Œ∏_old - (learning_rate) * (‚àÇJ/‚àÇŒ∏)

Where:
- J(Œ∏) ‚Üí cost function (e.g., MSE)
- ‚àÇJ/‚àÇŒ∏ ‚Üí gradient (slope) of cost function wrt parameter
- learning_rate ‚Üí controls step size

---

## Mathematical Behind Linear Regression Gradient Descent:
For simple linear regression:
    yÃÇ = m*x + b
Cost function (MSE):
    J = (1/N) * Œ£(y - yÃÇ)¬≤

Gradients:
    ‚àÇJ/‚àÇm = (-2/N) * Œ£ x * (y - yÃÇ)
    ‚àÇJ/‚àÇb = (-2/N) * Œ£ (y - yÃÇ)

We update both:
    m_new = m_old - Œ± * ‚àÇJ/‚àÇm
    b_new = b_old - Œ± * ‚àÇJ/‚àÇb

---

## Benefits:
‚úÖ Works even when analytical solution (like OLS) is hard to compute  
‚úÖ Can scale to millions of parameters (used in deep learning)  
‚úÖ Works on any differentiable cost function  

---

## Disadvantages:
‚ùå Can converge to local minima (for non-convex functions)  
‚ùå Requires careful tuning of learning rate  
‚ùå Slower than direct solutions like OLS for small problems  

"""

# ===============================================================
# üß† IMPORTS
# ===============================================================
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.animation import FuncAnimation
import matplotlib.animation as animation
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split

# ===============================================================
# üßÆ 1. DATA GENERATION
# ===============================================================
X, y = make_regression(
    n_samples=4, n_features=1, n_informative=1, n_targets=1,
    noise=80, random_state=13
)

plt.scatter(X, y)
plt.title("Sample Data for Regression")
plt.show()

# ===============================================================
# üßæ 2. APPLY ORDINARY LEAST SQUARES (OLS)
# ===============================================================
reg = LinearRegression()
reg.fit(X, y)

print("OLS Coefficient (m):", reg.coef_)
print("OLS Intercept (b):", reg.intercept_)

plt.scatter(X, y)
plt.plot(X, reg.predict(X), color='red', label='OLS Fit')
plt.legend()
plt.show()

# ===============================================================
# üîÅ 3. APPLY GRADIENT DESCENT FOR INTERCEPT (b)
# ===============================================================
m = 78.35  # constant slope (from OLS)
b = 100     # initial intercept
lr = 0.1    # learning rate

for i in range(3):
    loss_slope = -2 * np.sum(y - m * X.ravel() - b)
    b = b - lr * loss_slope
    print(f"Iteration {i+1}: b = {b}")

    plt.scatter(X, y)
    plt.plot(X, reg.predict(X), color='red', label='OLS')
    plt.plot(X, (m * X + b), color='#00a65a', label=f'b={b:.2f}')
    plt.legend()
    plt.show()

# ===============================================================
# üîÑ 4. ITERATIVE UPDATE (b only)
# ===============================================================
b = -100
m = 78.35
lr = 0.01
epochs = 100

for i in range(epochs):
    loss_slope = -2 * np.sum(y - m * X.ravel() - b)
    b = b - (lr * loss_slope)
    y_pred = m * X + b
    plt.plot(X, y_pred, alpha=0.2, color='green')

plt.scatter(X, y)
plt.title("Gradient Descent Convergence (b only)")
plt.show()

# ===============================================================
# üéûÔ∏è 5. ANIMATION OF GRADIENT DESCENT (b only)
# ===============================================================
X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=13)
reg = LinearRegression()
reg.fit(X, y)
print("OLS ‚Üí m:", reg.coef_, " | b:", reg.intercept_)

b = -150
m = 27.82
lr = 0.001
epochs = 30
all_b, all_cost = [], []

for i in range(epochs):
    slope, cost = 0, 0
    for j in range(X.shape[0]):
        slope = slope - 2 * (y[j] - (m * X[j]) - b)
        cost = cost + (y[j] - m * X[j] - b) ** 2
    b = b - (lr * slope)
    all_b.append(b)
    all_cost.append(cost)
    plt.plot(X, m * X + b, alpha=0.3)
plt.scatter(X, y)
plt.title("Gradient Descent Line Updates (b only)")
plt.show()

# ===============================================================
# üìà 6. ANIMATE COST FUNCTION
# ===============================================================
num_epochs = list(range(1, 31))
fig = plt.figure(figsize=(9, 5))
axis = plt.axes(xlim=(0, 31), ylim=(0, 2500000))
line, = axis.plot([], [], lw=2)
xdata, ydata = [], []

def animate(i):
    label = f'epoch {i+1}'
    xdata.append(num_epochs[i])
    ydata.append(all_cost[i])
    line.set_data(xdata, ydata)
    axis.set_xlabel(label)
    return line,

anim = animation.FuncAnimation(fig, animate, frames=30, repeat=False, interval=500)
plt.title("Cost Function Decrease Over Epochs")
plt.show()

# ===============================================================
# ‚öôÔ∏è 7. GRADIENT DESCENT FOR m AND b (FULL)
# ===============================================================
X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=13)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

# Scikit-learn comparison
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
print("Sklearn Coef (m):", lr_model.coef_)
print("Sklearn Intercept (b):", lr_model.intercept_)
print("R¬≤ Score (OLS):", r2_score(y_test, lr_model.predict(X_test)))

# ---------------------------------------------------------------
# Custom Gradient Descent Implementation
# ---------------------------------------------------------------
class GDRegressor:
    def __init__(self, learning_rate=0.001, epochs=50):
        self.m = 100
        self.b = -120
        self.lr = learning_rate
        self.epochs = epochs

    def fit(self, X, y):
        for i in range(self.epochs):
            loss_slope_b = -2 * np.sum(y - self.m * X.ravel() - self.b)
            loss_slope_m = -2 * np.sum((y - self.m * X.ravel() - self.b) * X.ravel())
            self.b -= self.lr * loss_slope_b
            self.m -= self.lr * loss_slope_m
        print(f"Final Parameters ‚Üí m = {self.m:.3f}, b = {self.b:.3f}")

    def predict(self, X):
        return self.m * X + self.b

gd = GDRegressor(learning_rate=0.001, epochs=50)
gd.fit(X_train, y_train)
y_pred = gd.predict(X_test)

print("R¬≤ Score (GD from scratch):", r2_score(y_test, y_pred))




# ------------------------------------------------------------
# üåü GRADIENT DESCENT ‚Äî Optimization Algorithm in ML
# ------------------------------------------------------------

# Gradient Descent is an optimization algorithm used to minimize
# the loss function by updating model parameters (weights)
# in the opposite direction of the gradient (slope) of the loss.

# ------------------------------------------------------------
# üîπ EFFECT OF LEARNING RATE
# ------------------------------------------------------------
# The learning rate (Œ±) controls the step size for each update.
# - Too high ‚Üí may overshoot the minimum (unstable training)
# - Too low ‚Üí may take too long to converge (slow training)
# - Optimal ‚Üí converges smoothly to the minimum

# Example:
# w = w - Œ± * (dL/dw)
# where Œ± = learning rate, dL/dw = derivative of loss w.r.t weight

# ------------------------------------------------------------
# üîπ EFFECT OF LOSS FUNCTION
# ------------------------------------------------------------
# The loss function measures how well the model predicts outputs.
# Gradient Descent works to minimize this loss.
# - Different problems need different loss functions:
#   - Regression ‚Üí Mean Squared Error (MSE)
#   - Classification ‚Üí Cross-Entropy Loss
# - A poor loss function choice can lead to bad convergence or
#   incorrect optimization direction.

# ------------------------------------------------------------
# üîπ EFFECT OF DATA
# ------------------------------------------------------------
# The quality and quantity of training data directly affect
# the gradient descent process.
# - Noisy data ‚Üí unstable gradients, poor convergence
# - Too small data ‚Üí may overfit quickly
# - Well-normalized & large dataset ‚Üí stable and smooth convergence

# ------------------------------------------------------------
# üîπ TYPES OF GRADIENT DESCENT
# ------------------------------------------------------------
# 1Ô∏è‚É£ Batch Gradient Descent
# 2Ô∏è‚É£ Stochastic Gradient Descent (SGD)
# 3Ô∏è‚É£ Mini-Batch Gradient Descent

# ------------------------------------------------------------
# üß© 1. BATCH GRADIENT DESCENT
# ------------------------------------------------------------
# - Uses the entire training dataset to compute the gradient.
# - Provides a stable and accurate gradient estimate.
# - Very slow for large datasets (requires high memory).
# - Use when dataset is small and fits into memory.

# Pseudocode:
# for epoch in range(num_epochs):
#     gradients = compute_gradients(X_train, y_train)
#     weights = weights - Œ± * gradients

# ‚úÖ Benefits:
# - Stable convergence
# - Accurate gradient estimation
# ‚ùå Disadvantages:
# - Computationally expensive
# - Slow for large datasets
# ‚öôÔ∏è Use Case:
# - Suitable for small datasets or offline batch training.

# ------------------------------------------------------------
# ‚ö° 2. STOCHASTIC GRADIENT DESCENT (SGD)
# ------------------------------------------------------------
# - Updates weights after each training sample.
# - Faster updates, introduces noise ‚Üí helps escape local minima.
# - Less stable than batch gradient descent.

# Pseudocode:
# for epoch in range(num_epochs):
#     for i in range(len(X_train)):
#         gradient = compute_gradient(X_train[i], y_train[i])
#         weights = weights - Œ± * gradient

# ‚úÖ Benefits:
# - Faster updates
# - Can handle large datasets
# - May escape local minima
# ‚ùå Disadvantages:
# - High variance in updates (noisy path)
# - May never converge exactly
# ‚öôÔ∏è Use Case:
# - Large datasets, online learning scenarios.

# ------------------------------------------------------------
# üî∏ 3. MINI-BATCH GRADIENT DESCENT
# ------------------------------------------------------------
# - Combines advantages of both Batch and SGD.
# - Uses small batches of data to compute gradients.
# - Balances speed and stability.

# Pseudocode:
# for epoch in range(num_epochs):
#     for batch in mini_batches(X_train, y_train, batch_size):
#         gradients = compute_gradients(batch)
#         weights = weights - Œ± * gradients

# ‚úÖ Benefits:
# - Faster convergence than batch
# - Less noisy than SGD
# - Efficient on GPUs
# ‚ùå Disadvantages:
# - Requires tuning batch size
# ‚öôÔ∏è Use Case:
# - Most commonly used in deep learning training.

# ------------------------------------------------------------
# üß† Summary:
# ------------------------------------------------------------
# - Batch Gradient Descent ‚Üí Stable but slow (small datasets)
# - Stochastic Gradient Descent ‚Üí Fast but noisy (large datasets)
# - Mini-Batch Gradient Descent ‚Üí Best balance (standard in DL)
