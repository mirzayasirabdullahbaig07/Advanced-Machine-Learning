# ==========================================
# 02_Regression Metrics (MAE, MSE, RMSE, RÂ², Adjusted RÂ²)
# ==========================================

# These metrics are used to evaluate the performance of regression models.
# The goal of regression evaluation metrics is to measure how close
# the predicted values are to the actual (true) values.

# ==========================================
# 1ï¸âƒ£ MEAN ABSOLUTE ERROR (MAE)
# ==========================================
# ğŸ”¹ What is MAE?
# Mean Absolute Error measures the average magnitude of errors in predictions,
# without considering their direction (positive or negative).
# It simply averages the absolute difference between predicted and actual values.

# ğŸ”¹ Why is it called â€œAbsoluteâ€?
# Because it uses the absolute difference |yáµ¢ - Å·áµ¢|, ensuring that negative errors
# do not cancel out positive ones.

# ğŸ”¹ Intuition:
# It tells you on average, how much your predictions deviate from the actual values.
# For example, MAE = 2 means predictions are off by 2 units on average.

# ğŸ”¹ Mathematical Formula:
#     MAE = (1/n) * Î£ |yáµ¢ - Å·áµ¢|
# where:
#     yáµ¢ = actual value
#     Å·áµ¢ = predicted value
#     n = total number of samples

# ğŸ”¹ Advantages:
# âœ… Easy to interpret and understand
# âœ… Not sensitive to large errors (less effect of outliers)

# ğŸ”¹ Disadvantages:
# âŒ Doesnâ€™t penalize large errors strongly
# âŒ Not differentiable at 0 (problem in gradient-based optimization)


# ==========================================
# 2ï¸âƒ£ MEAN SQUARED ERROR (MSE)
# ==========================================
# ğŸ”¹ What is MSE?
# Mean Squared Error measures the average squared difference between
# actual and predicted values.

# ğŸ”¹ Why â€œSquaredâ€?
# Because the errors are squared to make all values positive and to penalize
# larger errors more heavily.

# ğŸ”¹ Intuition:
# Squaring the errors gives more weight to large deviations,
# meaning if your model makes a few big mistakes, MSE increases sharply.

# ğŸ”¹ Mathematical Formula:
#     MSE = (1/n) * Î£ (yáµ¢ - Å·áµ¢)Â²

# ğŸ”¹ Advantages:
# âœ… Strongly penalizes large errors
# âœ… Smooth and differentiable (good for optimization)

# ğŸ”¹ Disadvantages:
# âŒ Sensitive to outliers (large errors dominate)
# âŒ Units are squared (harder to interpret in original scale)


# ==========================================
# 3ï¸âƒ£ ROOT MEAN SQUARED ERROR (RMSE)
# ==========================================
# ğŸ”¹ What is RMSE?
# RMSE is simply the square root of MSE. It brings the error back
# to the same unit as the original target variable.

# ğŸ”¹ Intuition:
# RMSE gives an idea of the standard deviation of residuals.
# It shows how concentrated the data is around the best-fit line.

# ğŸ”¹ Mathematical Formula:
#     RMSE = âˆš( (1/n) * Î£ (yáµ¢ - Å·áµ¢)Â² )

# ğŸ”¹ Advantages:
# âœ… Same unit as target variable
# âœ… Penalizes larger errors more

# ğŸ”¹ Disadvantages:
# âŒ Sensitive to outliers (like MSE)
# âŒ Slightly harder to interpret than MAE


# ==========================================
# 4ï¸âƒ£ RÂ² SCORE (Coefficient of Determination)
# ==========================================
# ğŸ”¹ What is RÂ²?
# RÂ² measures how well the regression model fits the data.
# It represents the proportion of variance in the dependent variable
# that is predictable from the independent variable(s).

# ğŸ”¹ Why called â€œR-squaredâ€?
# Because it is derived from the square of the correlation coefficient (r).

# ğŸ”¹ Intuition:
# RÂ² = 1 means perfect fit (model explains 100% of variance)
# RÂ² = 0 means the model does no better than predicting the mean
# RÂ² < 0 means the model performs worse than the mean

# ğŸ”¹ Mathematical Formula:
#     RÂ² = 1 - [Î£(yáµ¢ - Å·áµ¢)Â² / Î£(yáµ¢ - È³)Â²]
# where:
#     È³ = mean of actual values

# ğŸ”¹ Advantages:
# âœ… Easy to interpret (percentage of explained variance)
# âœ… Works well for comparing different models

# ğŸ”¹ Disadvantages:
# âŒ RÂ² always increases when new predictors are added (even if useless)
# âŒ Doesnâ€™t indicate if a model is unbiased


# ==========================================
# 5ï¸âƒ£ ADJUSTED RÂ² SCORE
# ==========================================
# ğŸ”¹ Why Adjusted RÂ²?
# Adding new features always increases RÂ² â€” even if they add no predictive power.
# Adjusted RÂ² fixes this by penalizing the addition of unnecessary variables.

# ğŸ”¹ Intuition:
# It adjusts RÂ² based on the number of predictors (p) and samples (n).

# ğŸ”¹ Mathematical Formula:
#     Adjusted RÂ² = 1 - [(1 - RÂ²) * (n - 1) / (n - p - 1)]
# where:
#     n = number of samples
#     p = number of independent variables

# ğŸ”¹ Advantages:
# âœ… Fair comparison between models with different numbers of predictors
# âœ… Prevents overfitting by penalizing irrelevant features

# ğŸ”¹ Disadvantages:
# âŒ Slightly harder to interpret
# âŒ Only valid for linear regression type models


# ==========================================
# 6ï¸âƒ£ CODE IMPLEMENTATION (Using Placement Dataset)
# ==========================================

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load dataset
df = pd.read_csv('placement.csv')

# Visualize the data
plt.scatter(df['cgpa'], df['package'])
plt.xlabel('CGPA')
plt.ylabel('Package (in LPA)')
plt.title('CGPA vs Package')
plt.show()

# Prepare data
X = df[['cgpa']]
y = df['package']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

# Train Linear Regression Model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Make Predictions
y_pred = lr.predict(X_test)

# ==========================================
# 7ï¸âƒ£ EVALUATE MODEL USING DIFFERENT METRICS
# ==========================================

print("MAE:", mean_absolute_error(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("RÂ² Score:", r2_score(y_test, y_pred))

# ==========================================
# 8ï¸âƒ£ MANUAL CALCULATION OF ADJUSTED RÂ²
# ==========================================
r2 = r2_score(y_test, y_pred)
n = X_test.shape[0]
p = X_test.shape[1]

adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))
print("Adjusted RÂ² Score:", adjusted_r2)


# ==========================================
# 9ï¸âƒ£ Visual Understanding of Adjusted RÂ²
# ==========================================

# Case 1: Add a random feature (no real relationship)
new_df1 = df.copy()
new_df1['random_feature'] = np.random.random(200)
X = new_df1[['cgpa', 'random_feature']]
y = new_df1['package']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

r2 = r2_score(y_test, y_pred)
adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - 2 - 1))
print("After adding random feature:")
print("RÂ² =", r2)
print("Adjusted RÂ² =", adjusted_r2)

# Case 2: Add a meaningful feature (related to target)
new_df2 = df.copy()
new_df2['iq'] = new_df2['package'] + (np.random.randint(-12, 12, 200) / 10)
X = new_df2[['cgpa', 'iq']]
y = new_df2['package']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

r2 = r2_score(y_test, y_pred)
adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - 2 - 1))
print("\nAfter adding meaningful feature:")
print("RÂ² =", r2)
print("Adjusted RÂ² =", adjusted_r2)


# ==========================================
# âœ… SUMMARY
# ==========================================
# Metric        | Measures                     | Penalizes Outliers | Range         | Best Value
# --------------------------------------------------------------------------------------------
# MAE           | Average absolute error        | âŒ No               | [0, âˆ)        | 0
# MSE           | Average squared error         | âœ… Yes              | [0, âˆ)        | 0
# RMSE          | Root of MSE                   | âœ… Yes              | [0, âˆ)        | 0
# RÂ² Score      | Variance explained            | âŒ No               | (-âˆ, 1]       | 1
# Adjusted RÂ²   | Penalized RÂ² for extra vars   | âŒ No               | (-âˆ, 1]       | 1

# ==========================================
# END OF REGRESSION METRICS EXPLANATION
# ==========================================
